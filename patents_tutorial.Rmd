---
title: "patents_tutorial"
author: "Prashant Garg & Michael Yeomans"
date: "2023-05-12"
output:
  pdf_document: default
  html_document: default
---

################################################
#
#         2023 ML Workshop
#
#  Predicting, evaluating, and interpreting
#
#
################################################

install libraries (only once)
```{r}
# Un-comment and run these once, if you haven't installed them before
install.packages("tidyverse") # useful everything
install.packages("ggrepel") # nice labels for plots
install.packages("glmnet") # supervised machine learning
install.packages("quanteda") # very useful text stuff
install.packages("textclean") # useful text stuff
install.packages("textdata") # useful text stuff
install.packages("pROC") # accuracy function
```

load libraries
```{r}
# Run these every time
library(tidyverse)
library(ggrepel)
library(glmnet)
library(quanteda)
library(textclean)
library(textdata)
library(syuzhet)
library(pROC)
```

Here I am loading separate R scripts that contain functions we will use.
make sure these are all in your Rstudio project
All in the main folder, along with this script!
```{r}
source("create_dfm.R") # our dfm function as a separate script
source("kendall_acc.R") # our accuracy function
source("vectorFunctions.R") # for word embeddings
```

load and explore data
```{r}
# patents data 
patents_data <- readRDS("patents_data.RDS")

# First thing - check variables
names(patents_data)
```

Calculate a 1-gram feature count matrix for the review data, with no dropped words
```{r}
dfm1<-create_dfm(patents_data$patent_abstract,
                ngrams=1,
                #min.prop=0,
                stop.words = FALSE)

dim(dfm1) # explore dimensions of the matrix 
```
More than 731 ngrams

most common words - obvious
```{r}
sort(colMeans(dfm1),decreasing=TRUE)[1:20]
```

least common words
```{r}
sort(colMeans(dfm1))[1:20]
```

Let's put this on a plot
```{r}
ngram_counts<-data.frame(word=colnames(dfm1),
                         frequency=colMeans(dfm1),
                         f_rank=rank(-colMeans(dfm1),
                                     ties.method="first"))

ngram_counts %>%
  ggplot(aes(x=f_rank,y=frequency)) +
  geom_point()
```

Let's write the words on the plot
```{r}
ngram_counts %>%
  ggplot(aes(x=f_rank,y=frequency,label=word)) +
  geom_text()
```

That's a mess... let's create a new column and pick only a few words to plot
```{r}
ngram_counts <- ngram_counts %>%
  mutate(word_label=ifelse(f_rank%in%c(1:10, # top 10 words
                                       sample(11:200,10), # ten random words from 11-100
                                       sample(201:1000,20), # twenty random words from 101-1000
                                       sample(1000:n(),20)), # twenty random words above 1000
                           word,""))

ngram_counts %>%
  ggplot(aes(x=f_rank,y=frequency,label=word_label)) +
  geom_text()
```
better, but still messy.


Let's get rid of the rarest words and plot again
```{r}
ngram_counts %>%
  filter(f_rank<1000) %>%
  ggplot(aes(x=f_rank,y=frequency,label=word_label)) +
  geom_text()
```
still some overlap.. we'll use the ggrepel package to space out the words

We'll also add the points for all the words, just to see
```{r}
ngram_counts %>%
  filter(f_rank<1000) %>%
  ggplot(aes(x=f_rank,y=frequency,label=word_label)) +
  geom_point(color="red") +
  geom_label_repel(max.overlaps=100,size=7,
                   nudge_x = 1,nudge_y = 1,
                   force=3) + 
  labs(x="Frequency Rank",y="Frequency per Document") +
  theme_bw() +
  theme(axis.title = element_text(size=24),
        axis.text = element_text(size=24))
```

(optional) save the plot
```{r}
# ggsave("word_freq.png")
```

# prediction

Ok, let's build a model to predict whether a patent is from a given CPC category

A - HUMAN NECESSITIES
B - PERFORMING OPERATIONS; TRANSPORTING
C - CHEMISTRY; METALLURGY
D - TEXTILES; PAPER
E - FIXED CONSTRUCTIONS
F - MECHANICAL ENGINEERING; LIGHTING; HEATING; WEAPONS; BLASTING
G - PHYSICS
H - ELECTRICITY

Here are the averages for each - we will predict category H, electricity, which is ~ 20% of the data
```{r}

patents_data %>%
  select(A:H) %>%
  summarize_all(mean) %>%
  round(3)

```

For your assignment, clean is a text variable. you will have to convert to numeric for the classifier
```{r}
table(patents_data$clean)

patents_data$clean_numeric=1*(patents_data$clean=="clean")
```

Let's only use 1-grams for now
```{r}
dfm2<-create_dfm(patents_data$patent_abstract,ngrams=1) %>%
  convert(to="data.frame") %>%
  select(-doc_id)

# let's explore the dimensions again
dim(dfm2)
```
666 unigrams..

Most common words in clean and dirty patents... 
```{r}
sort(colMeans(dfm2[patents_data$H==1,]),decreasing=T)[1:20]
```

lots of the same words!
```{r}
sort(colMeans(dfm2[patents_data$H==0,]),decreasing=T)[1:20]
```

##################################################

As we will discuss, we are not interested in effects of individual words
Instead, we care more about how all the words perform as a class

To do this, we will use the cv.glmnet() function to build a model

First, we need to split the data into training and testing samples
```{r}
train_split=sample(1:nrow(patents_data),round(nrow(patents_data)/2))

length(train_split)
```

create our prediction variables
```{r}
dfm3<-create_dfm(patents_data$patent_abstract,ngrams=1) %>%
  convert(to="data.frame") %>%
  select(-doc_id)


trainX<-dfm3 %>%
  slice(train_split) %>%
  as.matrix() # we have to convert to matrix format for glmnet

trainY<-patents_data %>%
  slice(train_split) %>%
  pull(H) # note - H is already numeric!

testX<-dfm3 %>% 
  slice(-train_split) %>%
  as.matrix() # we have to convert to matrix format for glmnet

testY<-patents_data %>%
  slice(-train_split) %>%
  pull(H)
```

Put training data into LASSO model (note - glmnet requires a matrix)
```{r}
lasso_mod<-cv.glmnet(x=trainX,y=trainY)
```

generate predictions for test data
```{r}
test_predict<-predict(lasso_mod,newx = testX)[,1]
```

Note that while the true answers are binary, the predictions are continuous
Always check these distributions!!
```{r}
# true values
hist(testY)
```

```{r}
# predicted values
hist(test_predict)
```

For now, let's just split the predictions in two, using the median
```{r}
test_predict_binary=ifelse(test_predict>median(test_predict),
                           1,0)
```

This should have the same values as testY
```{r}
hist(test_predict_binary)
```

and we can calculate accuracy from that
```{r}
round(100*mean(test_predict_binary==testY),3)
```


We can also calculate a "confusion matrix" to identify our errors
```{r}
table(test_predict_binary,testY)
```
We are making lots of one error, and very little of the other. How could you fix this?




#######################################################
Let's get a little better at interpreting the model

instead of splitting the dfm we can split the data itself
if we do that, we need the dfm_match function so that the two dfms have the same dimensions

(note we are also using bi-grams and tri-grams this time)
```{r}
patents_data_train<-patents_data[train_split,]
patents_data_test<-patents_data[-train_split,]

patents_data_dfm_train<-create_dfm(patents_data_train$patent_abstract,ngrams=2:3)

patents_data_dfm_test<-create_dfm(patents_data_test$patent_abstract,ngrams=2:3) %>%
  dfm_match(colnames(patents_data_dfm_train))
```


```{r}
patent_model <-glmnet::cv.glmnet(x=patents_data_dfm_train %>%
                               as.matrix(),
                             y=as.numeric(patents_data_train$H))

plot(patent_model)
```

Interpret with a coefficient plot
This plots coefficients from the model, as well as how frequently each feature is used in each patent
```{r}
plotDat<-patent_model %>%
  coef(s="lambda.min") %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  %>%
  # add ngram frequencies for plotting
  left_join(data.frame(ngram=colnames(patents_data_dfm_train),
                       freq=colMeans(patents_data_dfm_train)))

plotDat %>%
  mutate_at(vars(score,freq),~round(.,3))

plotDat %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient(low="blue",high="red")+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps = 30,force = 6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(.01,.05,.1,.2,.5,1,2,5))+
  theme_bw() +
  labs(x="Coefficient in Model",y="Uses per Patent description")+
  theme(legend.position = "none",
        axis.title=element_text(size=20),
        axis.text=element_text(size=16))
```

# Evaluate Accuracy

we are going to use a function that calculates non-parametric correlations
this function reports percentage accuracy, where 50% is random guessing
it is calculated across all possible pairs of observations

```{r}
test_ngram_predict<-predict(patent_model,
                            newx = patents_data_dfm_test %>%
                              as.matrix(),
                            s="lambda.min")[,1]

acc_ngram<-roc(patents_data_test$H,test_ngram_predict)

acc_ngram

plot(acc_ngram)
```

Find examples (create variables)
```{r}
# store predictions in data, calculate accuracy
patents_data_test<-patents_data_test %>%
  mutate(prediction=test_ngram_predict,
         error=abs(H-prediction),
         bias=H-prediction)

close_electricity<-patents_data_test %>%
  filter(H==1 & error<.5) %>% 
  select(patent_abstract,H,prediction, patent_title)

close_other<-patents_data_test %>%
  filter(H==0 & error<.5) %>%
  select(patent_abstract,H,prediction, patent_title)
```

let's look at these examples now
```{r}
# close_electricity
close_electricity %>%
  slice(1:2) %>%
  pull(patent_abstract)  # optionally, replace abstract with patent_title
```


```{r}
# close_other
close_other %>%
  slice(1:2) %>%
  pull(patent_abstract) # optionally, replace abstract with patent_title
```


Error analysis - find biggest misses
```{r}
patents_data_test %>%
  mutate(electric=ifelse(H==1,"electricity","other")) %>%
  ggplot(aes(x=prediction,group=electric,fill=electric)) +
  geom_density(alpha=.25) +
  xlim(0,1) +
  theme_bw()
```



```{r}
miss_electricity<-patents_data_test %>%
  filter(H==1 & prediction<.1) %>%
  slice(1:10)

miss_other<-patents_data_test %>%
  filter(H==0 & prediction>.4) %>%
  slice(1:10)
```


```{r}
miss_other%>%
  slice(1:5) %>%
  pull(patent_abstract)
```


```{r}
miss_other%>%
  slice(1:5) %>%
  select(A:H)
```


```{r}
miss_electricity%>%
  slice(1:5) %>%
  pull(patent_abstract)
```


```{r}
miss_electricity%>%
  slice(1:5) %>%
  select(A:H)
```

